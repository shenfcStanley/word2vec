{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "print (np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    \"\"\"Tokenize the corpus text.\n",
    "    :param corpus: list containing a string of text (example: [\"I like playing football with my friends\"])\n",
    "    :return corpus_tokenized: indexed list of words in the corpus, in the same order as the original corpus (the example above would return [[1, 2, 3, 4]])\n",
    "    :return V: size of vocabulary\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    corpus_tokenized = tokenizer.texts_to_sequences(corpus)\n",
    "    V = len(tokenizer.word_index)\n",
    "    return corpus_tokenized, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes=None):\n",
    "    \"\"\"Converts a class vector (integers) to binary class matrix.\n",
    "    E.g. for use with categorical_crossentropy.\n",
    "    # Arguments\n",
    "        y: class vector to be converted into a matrix\n",
    "            (integers from 0 to num_classes).\n",
    "        num_classes: total number of classes.\n",
    "    # Returns\n",
    "        A binary matrix representation of the input.\n",
    "    \"\"\"\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes))\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus2io(corpus_tokenized, V, window_size):\n",
    "    \"\"\"Converts corpus text into context and center words\n",
    "    # Arguments\n",
    "        corpus_tokenized: corpus text\n",
    "        window_size: size of context window\n",
    "    # Returns\n",
    "        context and center words (arrays)\n",
    "    \"\"\"\n",
    "    for words in corpus_tokenized:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            labels = []\n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            contexts.append([words[i]-1 for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            labels.append(word-1)\n",
    "            x = np_utils.to_categorical(contexts, V)\n",
    "            y = np_utils.to_categorical(labels, V)\n",
    "            yield (x, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " center word = [1. 0. 0. 0. 0. 0. 0.] \n",
      " context words =\n",
      " [[[0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0.]]]\n",
      "1 \n",
      " center word = [0. 1. 0. 0. 0. 0. 0.] \n",
      " context words =\n",
      " [[[1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]]]\n",
      "2 \n",
      " center word = [0. 0. 1. 0. 0. 0. 0.] \n",
      " context words =\n",
      " [[[1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]]]\n",
      "3 \n",
      " center word = [0. 0. 0. 1. 0. 0. 0.] \n",
      " context words =\n",
      " [[[0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]]]\n",
      "4 \n",
      " center word = [0. 0. 0. 0. 1. 0. 0.] \n",
      " context words =\n",
      " [[[0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]]\n",
      "5 \n",
      " center word = [0. 0. 0. 0. 0. 1. 0.] \n",
      " context words =\n",
      " [[[0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]]\n",
      "6 \n",
      " center word = [0. 0. 0. 0. 0. 0. 1.] \n",
      " context words =\n",
      " [[[0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "corpus = [\"I like playing football with my friends\"]\n",
    "corpus_tokenized, V = tokenize(corpus)\n",
    "for i, (x, y) in enumerate(corpus2io(corpus_tokenized, V, window_size)):\n",
    "    print(i, \"\\n center word =\", y, \"\\n context words =\\n\",x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Calculate softmax based probability for given input vector\n",
    "    # Arguments\n",
    "        x: numpy array/list\n",
    "    # Returns\n",
    "        softmax of input array\n",
    "    \"\"\"\n",
    "    #print('x',x)\n",
    "    #print('x max',np.max(x))\n",
    "    #print('x-max x',x-np.max(x))\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cbow(context, label, W1, W2, loss):\n",
    "    \"\"\"\n",
    "    Implementation of Continuous-Bag-of-Words Word2Vec model\n",
    "    :param context: all the context words (these represent the inputs)\n",
    "    :param label: the center word (this represents the label)\n",
    "    :param W1: weights from the input to the hidden layer\n",
    "    :param W2: weights from the hidden to the output layer\n",
    "    :param loss: float that represents the current value of the loss function\n",
    "    :return: updated weights and loss\n",
    "    \"\"\"\n",
    "    x = np.mean(context, axis=1)\n",
    "    h = np.dot(W1.T, x.reshape(x.shape[1], 1))\n",
    "    u = np.dot(W2.T, h)\n",
    "    y_pred = softmax(u)\n",
    "    \n",
    "    #print('y_pred',y_pred)\n",
    "  \n",
    "    e = -label.reshape(-1,1) + y_pred\n",
    "    \n",
    "    dW2 = np.outer(h, e)\n",
    "    dW1 = np.outer(x.reshape(x.shape[1], 1), np.dot(W2, e))\n",
    "    \n",
    "    new_W1 = W1 - eta * dW1\n",
    "    new_W2 = W2 - eta * dW2\n",
    "    \n",
    "    #print('u',u)\n",
    "    #print('label',label.reshape(-1,1))\n",
    "    #print(float(u[label == 1]))\n",
    "    print(np.log(np.sum(np.exp(u))))\n",
    "    \n",
    "    loss += -float(u[label == 1]) + np.log(np.sum(np.exp(u)))\n",
    "    \n",
    "    return new_W1, new_W2, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2763897061413036\n",
      "2.3515156186174426\n",
      "2.4900660445738034\n",
      "2.3465117434980844\n",
      "2.3292382124675455\n",
      "2.355451485905903\n",
      "2.390150300367837\n"
     ]
    }
   ],
   "source": [
    "#user-defined parameters\n",
    "corpus = [\"I like playing football with my friends\"] #our example text corpus\n",
    "N = 2 #assume that the hidden layer has dimensionality = 2\n",
    "window_size = 2 #symmetrical\n",
    "eta = 0.1 #learning rate\n",
    "corpus_tokenized, V = tokenize(corpus)\n",
    "#initialize weights (with random values) and loss function\n",
    "np.random.seed(100)\n",
    "W1 = np.random.rand(V, N)\n",
    "W2 = np.random.rand(N, V)\n",
    "loss = 0.\n",
    "for i, (context, label) in enumerate(corpus2io(corpus_tokenized, V, window_size)):\n",
    "    \n",
    "    \n",
    "    W1, W2, loss = cbow(context, label, W1, W2, loss)\n",
    "    #print(\"Training example #{} \\n-------------------- \\n\\n \\t label = {}, \\n \\t context = {}\".format(i, label, context))\n",
    "    #print(\"\\t W1 = {}\\n\\t W2 = {} \\n\\t loss = {}\\n\".format(W1, W2, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skipgram(context, x, W1, W2, loss):\n",
    "    \"\"\"\n",
    "    Implementation of Skip-Gram Word2Vec model\n",
    "    :param context: all the context words (these represent the labels)\n",
    "    :param label: the center word (this represents the input)\n",
    "    :param W1: weights from the input to the hidden layer\n",
    "    :param W2: weights from the hidden to the output layer\n",
    "    :param loss: float that represents the current value of the loss function\n",
    "    :return: updated weights and loss\n",
    "    \"\"\"\n",
    "    h = np.dot(W1.T, x.reshape(len(x), 1))\n",
    "    u = np.dot(W2.T, h)\n",
    "    y_pred = softmax(u)\n",
    "    \n",
    "    e = np.array([-label + y_pred.T for label in context])\n",
    "    dW2 = np.outer(h, np.sum(e, axis=0))\n",
    "    dW1 = np.outer(x.reshape(x.shape[0], 1), np.dot(W2, np.sum(e, axis=0).T))\n",
    "    \n",
    "    new_W1 = W1 - eta * dW1\n",
    "    new_W2 = W2 - eta * dW2\n",
    "    \n",
    "    loss += -np.sum([u[label == 1] for label in context]) + len(context) * np.log(np.sum(np.exp(u)))\n",
    "    \n",
    "    return new_W1, new_W2, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 7 but corresponding boolean dimension is 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-f91cee8c86f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskipgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training example #{} \\n-------------------- \\n\\n \\t label = {}, \\n \\t center = {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t W1 = {}\\n\\t W2 = {} \\n\\t loss = {}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-183-9493ef5f0dd1>\u001b[0m in \u001b[0;36mskipgram\u001b[0;34m(context, x, W1, W2, loss)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mnew_W2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdW2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_W1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_W2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-183-9493ef5f0dd1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mnew_W2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdW2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_W1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_W2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 7 but corresponding boolean dimension is 2"
     ]
    }
   ],
   "source": [
    "for i, (label, center) in enumerate(corpus2io(corpus_tokenized, V, window_size)):\n",
    "    \n",
    "    \n",
    "    W1, W2, loss = skipgram(label, center, W1, W2, loss)\n",
    "    print(\"Training example #{} \\n-------------------- \\n\\n \\t label = {}, \\n \\t center = {}\".format(i, label, center))\n",
    "    print(\"\\t W1 = {}\\n\\t W2 = {} \\n\\t loss = {}\\n\".format(W1, W2, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
